# app/services/unity_text_processor.py
from langchain.text_splitter import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
import numpy as np
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)

class UnityTextProcessor:
    def __init__(self):
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        try:
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.embedding_model = None
        
        # é’ˆå¯¹Unityä»£ç çš„æ™ºèƒ½åˆ†å‰²å™¨
        self.code_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            length_function=len,
            separators=[
                '\nclass ', '\npublic class ', '\n[System.Serializable]',
                '\npublic ', '\nprivate ', '\nprotected ', '\nvoid ',
                '\nfunction ', '\n#region ', '\n#endregion ', '\n// ----',
                '\n\n', '\n', ' ', ''
            ]
        )
        
        # é’ˆå¯¹é…ç½®æ–‡ä»¶çš„é€šç”¨åˆ†å‰²å™¨
        self.config_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            length_function=len
        )
    
    def split_unity_documents(self, documents: List[Dict]) -> List[Dict]:
        """åˆ†å‰²Unityæ–‡æ¡£"""
        chunks = []
        
        print("âœ‚ï¸ å¼€å§‹åˆ†å‰²Unityæ–‡æ¡£...")
        
        for doc in documents:
            content = doc['content']
            metadata = doc['metadata']
            file_type = metadata['file_type']
            
            try:
                if file_type == 'code':
                    code_chunks = self._split_code_file(content, metadata)
                    chunks.extend(code_chunks)
                elif file_type in ['scene', 'prefab']:
                    yaml_chunks = self._split_yaml_file(content, metadata)
                    chunks.extend(yaml_chunks)
                else:
                    # é€šç”¨åˆ†å‰²
                    text_chunks = self.config_splitter.split_text(content)
                    for i, chunk in enumerate(text_chunks):
                        chunks.append(self._create_chunk(chunk, metadata, i))
                        
            except Exception as e:
                print(f"âš ï¸ åˆ†å‰²æ–‡æ¡£å¤±è´¥ {metadata['file_path']}: {e}")
        
        print(f"âœ… æ–‡æ¡£åˆ†å‰²å®Œæˆ: {len(chunks)} ä¸ªå—")
        return chunks
    
    def generate_embeddings(self, chunks: List[Dict]) -> np.ndarray:
        """ç”Ÿæˆæ–‡æœ¬å—çš„åµŒå…¥å‘é‡"""
        print("step1")
        if self.embedding_model is None:
            raise RuntimeError("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å®‰è£…: pip install sentence-transformers")
        
        texts = [chunk['content'] for chunk in chunks]
        print(f"ğŸ§  ä¸º {len(texts)} ä¸ªæ–‡æœ¬å—ç”ŸæˆåµŒå…¥å‘é‡...")
        
        try:
            embeddings = self.embedding_model.encode(
                texts, 
                show_progress_bar=True,
                batch_size=32,
                convert_to_numpy=True
            )
            print(f"âœ… åµŒå…¥å‘é‡ç”Ÿæˆå®Œæˆ: {embeddings.shape}")
            return embeddings
            
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆåµŒå…¥å‘é‡å¤±è´¥: {e}")
            # è¿”å›éšæœºåµŒå…¥å‘é‡ä½œä¸ºå¤‡é€‰
            print("âš ï¸ ä½¿ç”¨éšæœºåµŒå…¥å‘é‡ä½œä¸ºå¤‡é€‰")
            return np.random.randn(len(texts), 384).astype('float32')
    
    def _split_code_file(self, content: str, metadata: Dict) -> List[Dict]:
        """åˆ†å‰²ä»£ç æ–‡ä»¶"""
        chunks = []
        
        # ä½¿ç”¨ä¸“é—¨çš„åˆ†å‰²å™¨
        text_chunks = self.code_splitter.split_text(content)
        
        for i, chunk in enumerate(text_chunks):
            chunk_metadata = metadata.copy()
            chunk_metadata.update({
                'chunk_type': 'code_block',
                'chunk_index': i,
                'block_type': self._detect_block_type(chunk)
            })
            
            chunks.append({
                'content': chunk,
                'metadata': chunk_metadata
            })
        
        return chunks
    
    def _split_yaml_file(self, content: str, metadata: Dict) -> List[Dict]:
        """åˆ†å‰²YAMLæ–‡ä»¶ï¼ˆåœºæ™¯ã€é¢„åˆ¶ä½“ï¼‰"""
        chunks = []
        lines = content.split('\n')
        
        current_chunk = []
        current_section = "header"
        
        for line in lines:
            # æ£€æµ‹æ–°çš„YAMLæ–‡æ¡£å¼€å§‹
            if line.strip() == '---' and current_chunk:
                # ä¿å­˜å½“å‰å—
                if len(current_chunk) > 3:  # è¿‡æ»¤å¤ªå°çš„å—
                    chunk_content = '\n'.join(current_chunk)
                    chunk_metadata = metadata.copy()
                    chunk_metadata.update({
                        'chunk_type': 'yaml_document',
                        'section': current_section
                    })
                    chunks.append({
                        'content': chunk_content,
                        'metadata': chunk_metadata
                    })
                
                # å¼€å§‹æ–°å—
                current_chunk = [line]
                current_section = "new_document"
            else:
                current_chunk.append(line)
                
                # æ›´æ–°å½“å‰ç« èŠ‚ç±»å‹
                if line.strip().startswith('GameObject:'):
                    current_section = "game_object"
                elif line.strip().startswith('MonoBehaviour:'):
                    current_section = "component"
        
        # å¤„ç†æœ€åä¸€ä¸ªå—
        if current_chunk and len(current_chunk) > 3:
            chunk_content = '\n'.join(current_chunk)
            chunk_metadata = metadata.copy()
            chunk_metadata.update({
                'chunk_type': 'yaml_document',
                'section': current_section
            })
            chunks.append({
                'content': chunk_content,
                'metadata': chunk_metadata
            })
        
        return chunks
    
    def _detect_block_type(self, chunk: str) -> str:
        """æ£€æµ‹ä»£ç å—ç±»å‹"""
        lines = chunk.split('\n')
        first_line = lines[0].strip() if lines else ""
        
        if first_line.startswith('class '):
            return 'class_definition'
        elif first_line.startswith('public ') or first_line.startswith('private '):
            if '(' in first_line and ')' in first_line:
                return 'method_definition'
            else:
                return 'field_definition'
        elif first_line.startswith('void ') or first_line.startswith('IEnumerator '):
            return 'method_definition'
        elif first_line.startswith('using '):
            return 'using_directive'
        elif first_line.startswith('//') or first_line.startswith('/*'):
            return 'comment'
        else:
            return 'code_block'
    
    def _create_chunk(self, content: str, metadata: Dict, chunk_index: int) -> Dict:
        """åˆ›å»ºæ–‡æœ¬å—"""
        chunk_metadata = metadata.copy()
        chunk_metadata.update({
            'chunk_type': 'text_block',
            'chunk_index': chunk_index
        })
        
        return {
            'content': content,
            'metadata': chunk_metadata
        }