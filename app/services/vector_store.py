# app/services/vector_store.py
import chromadb
from chromadb.config import Settings
import numpy as np
from typing import List, Dict, Optional, Any
import os
import logging

logger = logging.getLogger(__name__)

class ChromaVectorStore:
    def __init__(self, persist_directory: str = "./chroma_db"):
        """åˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“"""
        self.persist_directory = persist_directory
        os.makedirs(persist_directory, exist_ok=True)
        
        try:
            self.client = chromadb.PersistentClient(
                path=persist_directory,
                settings=Settings(anonymized_telemetry=False)
            )
            self.collection = None
            logger.info(f"âœ… Chromaå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ: {persist_directory}")
        except Exception as e:
            logger.error(f"âŒ Chromaåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def create_collection(self, collection_name: str = "unity_project"):
        """åˆ›å»ºæˆ–è·å–é›†åˆ"""
        try:
            # å°è¯•è·å–ç°æœ‰é›†åˆ
            self.collection = self.client.get_collection(collection_name)
            logger.info(f"âœ… åŠ è½½ç°æœ‰é›†åˆ: {collection_name}")
        except Exception:
            # åˆ›å»ºæ–°é›†åˆ
            self.collection = self.client.create_collection(
                name=collection_name,
                metadata={"description": "Unity project code and documentation"}
            )
            logger.info(f"âœ… åˆ›å»ºæ–°é›†åˆ: {collection_name}")
    
    # def add_documents(self, chunks: List[Dict], embeddings: np.ndarray):
    #     """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
    #     if not self.collection:
    #         self.create_collection()
        
    #     logger.info("ğŸ’¾ ä¿å­˜æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...")
        
    #     documents = []
    #     metadatas = []
    #     ids = []
        
    #     for i, chunk in enumerate(chunks):
    #         # é™åˆ¶æ–‡æ¡£é•¿åº¦ï¼Œé¿å…è¿‡é•¿
    #         content = chunk['content']
    #         if len(content) > 10000:  # é™åˆ¶æœ€å¤§é•¿åº¦
    #             content = content[:10000] + "\n... [å†…å®¹æˆªæ–­]"
            
    #         documents.append(content)
    #         metadatas.append(chunk['metadata'])
            
    #         # ç”Ÿæˆå”¯ä¸€ID
    #         file_path = chunk['metadata'].get('file_path', 'unknown')
    #         chunk_id = f"chunk_{i}_{hash(file_path) % 10000:04d}"
    #         ids.append(chunk_id)
        
    #     try:
    #         # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
    #         embeddings_list = embeddings.tolist()
            
    #         self.collection.add(
    #             embeddings=embeddings_list,
    #             documents=documents,
    #             metadatas=metadatas,
    #             ids=ids
    #         )
            
    #         logger.info(f"ğŸ‰ å‘é‡æ•°æ®åº“æ›´æ–°å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£")
            
    #     except Exception as e:
    #         logger.error(f"âŒ æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
    #         raise
    
    # def add_documents(self, chunks: List[Dict], embeddings: np.ndarray):
    #     """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“"""
    #     if not self.collection:
    #         self.create_collection()
        
    #     logger.info("ğŸ’¾ ä¿å­˜æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“...")
        
    #     documents = []
    #     metadatas = []
    #     ids = []
        
    #     for i, chunk in enumerate(chunks):
    #         # é™åˆ¶æ–‡æ¡£é•¿åº¦ï¼Œé¿å…è¿‡é•¿
    #         content = chunk['content']
    #         if len(content) > 10000:  # é™åˆ¶æœ€å¤§é•¿åº¦
    #             content = content[:10000] + "\n... [å†…å®¹æˆªæ–­]"
            
    #         documents.append(content)
            
    #         # æ¸…ç†metadataï¼Œç¡®ä¿åªåŒ…å«åŸºæœ¬æ•°æ®ç±»å‹
    #         metadata = self._clean_metadata(chunk['metadata'])
    #         metadatas.append(metadata)
            
    #         # ç”Ÿæˆå”¯ä¸€ID
    #         file_path = chunk['metadata'].get('file_path', 'unknown')
    #         chunk_id = f"chunk_{i}_{hash(file_path) % 10000:04d}"
    #         ids.append(chunk_id)
        
    #     try:
    #         # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
    #         embeddings_list = embeddings.tolist()
            
    #         self.collection.add(
    #             embeddings=embeddings_list,
    #             documents=documents,
    #             metadatas=metadatas,
    #             ids=ids
    #         )
            
    #         logger.info(f"ğŸ‰ å‘é‡æ•°æ®åº“æ›´æ–°å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£")
            
    #     except Exception as e:
    #         logger.error(f"âŒ æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
    #         raise
    def add_documents(self, chunks, embeddings):
        """æ·»åŠ æ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“"""
        try:
            # å‡†å¤‡æ–‡æ¡£æ•°æ®
            documents = []
            metadatas = []
            ids = []
            
            for i, chunk in enumerate(chunks):
                # æ–¹æ³•1ï¼šå¦‚æœ chunk æ˜¯å­—å…¸
                if isinstance(chunk, dict):
                    text = chunk.get('text', '') or chunk.get('content', '')
                    metadata = chunk.get('metadata', {})
                # æ–¹æ³•2ï¼šå¦‚æœ chunk æœ‰ text å±æ€§
                elif hasattr(chunk, 'text'):
                    text = chunk.text
                    metadata = getattr(chunk, 'metadata', {})
                else:
                    logger.warning(f"âš ï¸ æ— æ³•å¤„ç†çš„ chunk ç±»å‹: {type(chunk)}")
                    continue

                if not text:
                    logger.warning(f"âš ï¸ è·³è¿‡ç©ºæ–‡æœ¬çš„ chunk {i}")
                    continue

                #documents.append(chunk.text)
                documents.append(text)
                
                # æ¸…ç†å…ƒæ•°æ®ï¼Œç¡®ä¿æ²¡æœ‰ None å€¼
                cleaned_metadata = {}
                #if (not isinstance(chunk, dict)) and hasattr(chunk, 'metadata'):
                if metadata:#chunk.metadata:
                    for key, value in metadata.items():#chunk.metadata.items():
                        if value is not None:
                            # æ ¹æ®å€¼çš„ç±»å‹è¿›è¡Œé€‚å½“è½¬æ¢
                            if isinstance(value, (str, int, float, bool)):
                                cleaned_metadata[key] = value
                            else:
                                # å°†å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                                cleaned_metadata[key] = str(value)
                        else:
                            # å¯¹äº None å€¼ï¼Œæä¾›é»˜è®¤å€¼æˆ–è·³è¿‡
                            cleaned_metadata[key] = ""  # æˆ–è€…è·³è¿‡è¿™ä¸ªå­—æ®µ
                    
                metadatas.append(cleaned_metadata)
                ids.append(f"chunk_{i}")
            
            # è½¬æ¢ä¸ºåµŒå…¥å‘é‡åˆ—è¡¨
            embeddings_list = embeddings.tolist()

            self.collection.add(
                embeddings=embeddings_list,
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            
            logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            raise

    def _clean_metadata(self, metadata: Dict) -> Dict:
        """æ¸…ç†metadataï¼Œç¡®ä¿åªåŒ…å«ChromaDBæ”¯æŒçš„æ•°æ®ç±»å‹"""
        cleaned = {}
        
        for key, value in metadata.items():
            if value is None:
                cleaned[key] = None
            elif isinstance(value, (str, int, float, bool)):
                cleaned[key] = value
            elif isinstance(value, list):
                # å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                cleaned[key] = ", ".join(str(item) for item in value)
            elif isinstance(value, dict):
                # å°†å­—å…¸è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                import json
                try:
                    cleaned[key] = json.dumps(value)
                except:
                    cleaned[key] = str(value)
            else:
                # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                cleaned[key] = str(value)
        
        return cleaned

        def search(self, query: str, n_results: int = 5, 
                  where_filter: Optional[Dict] = None) -> List[Dict]:
            """æœç´¢ç›¸å…³æ–‡æ¡£"""
            if not self.collection:
                return []
            
            try:
                results = self.collection.query(
                    query_texts=[query],
                    n_results=n_results,
                    where=where_filter
                )
                
                formatted_results = []
                if results['documents'] and len(results['documents'][0]) > 0:
                    for i in range(len(results['documents'][0])):
                        formatted_results.append({
                            'content': results['documents'][0][i],
                            'metadata': results['metadatas'][0][i],
                            'distance': results['distances'][0][i] if results['distances'] else 0,
                            'score': 1 - (results['distances'][0][i] if results['distances'] else 0)
                        })
                
                logger.info(f"ğŸ” æœç´¢å®Œæˆ: æŸ¥è¯¢='{query}', ç»“æœæ•°={len(formatted_results)}")
                return formatted_results
                
            except Exception as e:
                logger.error(f"âŒ æœç´¢å¤±è´¥: {e}")
                return []
        
    def search_by_embedding(self, embedding: np.ndarray, n_results: int = 5) -> List[Dict]:
        """é€šè¿‡åµŒå…¥å‘é‡æœç´¢"""
        if not self.collection:
            return []
        
        try:
            results = self.collection.query(
                query_embeddings=[embedding.tolist()],
                n_results=n_results
            )
            
            formatted_results = []
            if results['documents'] and len(results['documents'][0]) > 0:
                for i in range(len(results['documents'][0])):
                    formatted_results.append({
                        'content': results['documents'][0][i],
                        'metadata': results['metadatas'][0][i],
                        'distance': results['distances'][0][i],
                        'score': 1 - results['distances'][0][i]
                    })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"âŒ å‘é‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def get_collection_info(self) -> Dict:
        """è·å–é›†åˆä¿¡æ¯"""
        if not self.collection:
            return {}
        
        try:
            count = self.collection.count()
            return {
                'document_count': count,
                'name': self.collection.name,
                'persist_directory': self.persist_directory
            }
        except Exception as e:
            logger.error(f"âŒ è·å–é›†åˆä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def delete_collection(self, collection_name: str):
        """åˆ é™¤é›†åˆ"""
        try:
            self.client.delete_collection(collection_name)
            logger.info(f"ğŸ—‘ï¸ åˆ é™¤é›†åˆ: {collection_name}")
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤é›†åˆå¤±è´¥: {e}")
    
    def list_collections(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰é›†åˆ"""
        try:
            collections = self.client.list_collections()
            return [col.name for col in collections]
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºé›†åˆå¤±è´¥: {e}")
            return []


# ç®€å•çš„å†…å­˜å‘é‡å­˜å‚¨ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
class SimpleVectorStore:
    """ç®€å•çš„å†…å­˜å‘é‡å­˜å‚¨ï¼Œç”¨äºæµ‹è¯•æˆ–å¤‡é€‰"""
    
    def __init__(self):
        self.documents = []
        self.embeddings = []
        self.metadatas = []
    
    def add_documents(self, chunks: List[Dict], embeddings: np.ndarray):
        """æ·»åŠ æ–‡æ¡£åˆ°å†…å­˜å­˜å‚¨"""
        self.documents = [chunk['content'] for chunk in chunks]
        self.metadatas = [chunk['metadata'] for chunk in chunks]
        self.embeddings = embeddings.tolist()
    
    def search(self, query: str, n_results: int = 5, where_filter: Optional[Dict] = None) -> List[Dict]:
        """ç®€å•æœç´¢ï¼ˆåŸºäºå…³é”®è¯åŒ¹é…ï¼‰"""
        # è¿™é‡Œå®ç°ç®€å•çš„å…³é”®è¯åŒ¹é…
        # å®é™…ä½¿ç”¨æ—¶åº”è¯¥ç”¨çœŸæ­£çš„å‘é‡æœç´¢
        query_lower = query.lower()
        results = []
        
        for i, doc in enumerate(self.documents):
            score = 0
            # ç®€å•çš„å…³é”®è¯åŒ¹é…è¯„åˆ†
            for word in query_lower.split():
                if word in doc.lower():
                    score += 1
            
            if score > 0:
                results.append({
                    'content': doc,
                    'metadata': self.metadatas[i],
                    'score': min(score / len(query.split()), 1.0)
                })
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:n_results]